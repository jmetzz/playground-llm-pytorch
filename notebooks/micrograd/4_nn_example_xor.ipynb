{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the XOR problem with a MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo $PATH=['/Users/jeanmetz/workspace/playground-llm-pytorch/src', '/Users/jeanmetz/.pyenv/versions/3.12.2/lib/python312.zip', '/Users/jeanmetz/.pyenv/versions/3.12.2/lib/python3.12', '/Users/jeanmetz/.pyenv/versions/3.12.2/lib/python3.12/lib-dynload', '', '/Users/jeanmetz/workspace/playground-llm-pytorch/.venv/lib/python3.12/site-packages']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x119ca7bf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# general imports and configuration\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "WORKSPACE_PATH = Path.cwd().parent.parent\n",
    "SRC_PATH = str(WORKSPACE_PATH / \"src\")\n",
    "\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH)\n",
    "    print(f\"echo $PATH={sys.path}\")\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "my_random_seed = 1337\n",
    "np.random.seed(my_random_seed)\n",
    "random.seed(my_random_seed)\n",
    "torch.manual_seed(my_random_seed)  # Set seed for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with the pytorch implementation (for reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchXor(nn.Module):\n",
    "    \"\"\"\n",
    "    x1 ─┐\n",
    "          │\n",
    "    x2 ─┼─> [ Hidden Neuron 1 (ReLU/tanh) ] ─┐\n",
    "          │                                   │\n",
    "          └─> [ Hidden Neuron 2 (ReLU/tanh) ] ─┘\n",
    "                                            │\n",
    "                                            ▼\n",
    "                                     [ Output Neuron (Sigmoid) ]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TorchXor, self).__init__()\n",
    "        # Input layer to hidden layer\n",
    "        # self.hidden = nn.Linear(2, 4)  # 2 inputs, 4 neurons in hidden layer\n",
    "        # # Hidden layer to output layer\n",
    "        # self.output = nn.Linear(4, 1)  # 4 neurons in hidden layer, 1 output\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(2, 4),  # 2 inputs, 4 neurons in hidden layer\n",
    "            nn.Tanh(),  # Add non-linearity to the network\n",
    "            nn.Linear(4, 1),  # 4 neurons in hidden layer, 1 output\n",
    "            nn.Sigmoid(),  # Activation for output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.tanh(self.hidden(x))  # Activation for hidden layer\n",
    "        # x = torch.sigmoid(self.output(x))  # Activation for output layer\n",
    "        # return x\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model, define loss function and optimizer\n",
    "model = TorchXor()\n",
    "lr = 0.1\n",
    "epochs = 1001\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                    [-1, 4]              12\n",
      "              Tanh-2                    [-1, 4]               0\n",
      "            Linear-3                    [-1, 1]               5\n",
      "           Sigmoid-4                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 17\n",
      "Trainable params: 17\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (2,))  # (2,) is the shape of the input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR data\n",
    "inputs = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "targets = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1000], Loss: 0.2552\n",
      "Epoch [100/1000], Loss: 0.2507\n",
      "Epoch [200/1000], Loss: 0.2483\n",
      "Epoch [300/1000], Loss: 0.2460\n",
      "Epoch [400/1000], Loss: 0.2431\n",
      "Epoch [500/1000], Loss: 0.2388\n",
      "Epoch [600/1000], Loss: 0.2321\n",
      "Epoch [700/1000], Loss: 0.2220\n",
      "Epoch [800/1000], Loss: 0.2079\n",
      "Epoch [900/1000], Loss: 0.1906\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    predictions = model(inputs)\n",
    "    loss = criterion(predictions, targets)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # update step\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted outputs for XOR inputs:\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(inputs)\n",
    "    print(\"\\nPredicted outputs for XOR inputs:\")\n",
    "    print(test_outputs.round())  # Round to 0 or 1 for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference results:\n",
      "Input: [0. 0.], Output: 0.0\n",
      "Input: [0. 1.], Output: 1.0\n",
      "Input: [1. 0.], Output: 1.0\n",
      "Input: [1. 1.], Output: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Let's try inference\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the input data (same as training XOR inputs here, but could be new data)\n",
    "test_inputs = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():  # No need for gradients during inference\n",
    "    test_outputs = model(test_inputs)\n",
    "    predicted = test_outputs.round()  # Round to 0 or 1 for binary classification\n",
    "\n",
    "print(\"Inference results:\")\n",
    "for i, input in enumerate(test_inputs):\n",
    "    print(f\"Input: {input.numpy()}, Output: {predicted[i].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build XOR model using the micrograd engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import micrograd\n",
    "import micrograd.nn\n",
    "from micrograd.engine import Operand\n",
    "from typing import Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple neuron class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MicrogradXor(micrograd.nn.Module):\n",
    "    \"\"\"\n",
    "    This implementation should mimic pytorch's\n",
    "    model = nn.Sequential(\n",
    "            nn.Linear(2, 4),  # 2 inputs, 4 neurons in hidden layer\n",
    "            nn.Tanh(),        # Add non-linearity to the network\n",
    "            nn.Linear(4, 1),  # 4 neurons in hidden layer, 1 output\n",
    "            nn.Sigmoid(),     # Activation for output layer\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        # Input layer to hidden LINEAR layer\n",
    "        self.hidden = micrograd.nn.Layer(2, 4)  # 2 inputs, 4 neurons in hidden layer\n",
    "        # Hidden layer to output LINEAR layer\n",
    "        self.output = micrograd.nn.Layer(4, 1)  # 4 neurons in hidden layer, 1 output\n",
    "\n",
    "    def __call__(self, inputs: Sequence[Operand | int | float]) -> Sequence[Operand]:\n",
    "        x = self.hidden(inputs)\n",
    "        x = [v.tanh() for v in x]  # apply non-linearity\n",
    "        x = self.output(x)\n",
    "        return x.sigmoid()\n",
    "\n",
    "    def parameters(\n",
    "        self,\n",
    "    ):\n",
    "        return [*self.hidden.parameters(), *self.output.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        hidden_size = len(self.hidden.neurons)\n",
    "        output_size = len(self.output.neurons)\n",
    "        return f\"MLP of [Linear-1 [-1, {hidden_size}], Tanh-2 [-1, {hidden_size}], Linear-3 [-1, {output_size}], Sigmore-4 [-1, {output_size}]]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(predictions: Sequence[Operand], targets: Sequence[Operand]) -> float:\n",
    "    squared_diffs = sum((p - t) ** 2 for p, t in zip(predictions, targets, strict=False))\n",
    "    return squared_diffs / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "targets = [0, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network\n",
      "Architecture:MLP of [Linear-1 [-1, 4], Tanh-2 [-1, 4], Linear-3 [-1, 1], Sigmore-4 [-1, 1]]\n",
      "Size: 17\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "micrograd_model = MicrogradXor()\n",
    "\n",
    "print(f\"The network\")\n",
    "print(f\"Architecture:{micrograd_model}\")\n",
    "print(f\"Size: {len(micrograd_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1001], Loss: 0.2438\n",
      "Epoch [100/1001], Loss: 0.2271\n",
      "Epoch [200/1001], Loss: 0.2111\n",
      "Epoch [300/1001], Loss: 0.1932\n",
      "Epoch [400/1001], Loss: 0.1770\n",
      "Epoch [500/1001], Loss: 0.1637\n",
      "Epoch [600/1001], Loss: 0.1543\n",
      "Epoch [700/1001], Loss: 0.1480\n",
      "Epoch [800/1001], Loss: 0.1433\n",
      "Epoch [900/1001], Loss: 0.1400\n",
      "Epoch [1000/1001], Loss: 0.1378\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 1001\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    predictions = [micrograd_model(x) for x in inputs]\n",
    "    loss = mse_loss(predictions, targets)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    micrograd_model.zero_grad()  # zero the gradients first\n",
    "    loss.backward()\n",
    "\n",
    "    # update step\n",
    "    for p in micrograd_model.parameters():\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch [{epoch}/{epochs}], Loss: {loss.data:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference results:\n",
      "Input: [0, 0], Output: 0.0\n",
      "Input: [0, 1], Output: 1.0\n",
      "Input: [1, 0], Output: 1.0\n",
      "Input: [1, 1], Output: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Define the input data (same as training XOR inputs here, but could be new data)\n",
    "test_inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "\n",
    "# Perform inference\n",
    "test_predictions = [micrograd_model(x) for x in test_inputs]\n",
    "# Round to 0 or 1 for binary classification\n",
    "test_predictions = torch.tensor([o.data for o in test_predictions]).round()\n",
    "\n",
    "print(\"Inference results:\")\n",
    "for i, pred in zip(test_inputs, test_predictions):\n",
    "    print(f\"Input: {i}, Output: {pred.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
